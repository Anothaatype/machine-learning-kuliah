{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b72920d",
   "metadata": {},
   "source": [
    "# *Question.*\n",
    "1. Construct an SVM model using the data voice.csv under the following conditions,\n",
    "\n",
    "    a. Split the data using ratios of 70:30 and 80:20 for each model to be developed.\n",
    "        - Use a model with a linear kernel.\n",
    "        - Use a model with a polynomial kernel.\n",
    "        - Use a model with an RBF kernel.\n",
    "    b. Tabulate the performance of each split and kernel based on the accuracy metric.\n",
    "2. Use the data from practical session 5 to develop a daytime and nighttime classification model using an SVM with an RBF kernel employing histogram features. Use an 80:20 ratio. You may experiment with hyperparameter tuning of the RBF kernel. Record the accuracy performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22f138",
   "metadata": {},
   "source": [
    "## **Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99ace9",
   "metadata": {},
   "source": [
    "### Step 1 - Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ec7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9099e0",
   "metadata": {},
   "source": [
    "### Step 2 - Voice Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c435be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 1: VOICE CLASSIFICATION USING SVM\n",
      "================================================================================\n",
      "\n",
      "[STEP 0] Loading voice.csv dataset...\n",
      "Dataset shape: (3168, 21)\n",
      "Number of features: 20\n",
      "Target column: 'label'\n",
      "\n",
      "First 5 rows:\n",
      "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "\n",
      "          kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
      "0   274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n",
      "1   634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n",
      "2  1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n",
      "3     4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n",
      "4     4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n",
      "\n",
      "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
      "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
      "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
      "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
      "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
      "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "male      1584\n",
      "female    1584\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Features shape: (3168, 20)\n",
      "Target shape: (3168,)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TASK 1: VOICE CLASSIFICATION USING SVM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 0: Load the dataset\n",
    "print(\"\\n[STEP 0] Loading voice.csv dataset...\")\n",
    "df = pd.read_csv(\"voice.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1}\")\n",
    "print(f\"Target column: '{df.columns[-1]}'\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df.iloc[:, -1].value_counts())\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = df.iloc[:, :-1]  # All columns except the last one\n",
    "y = df.iloc[:, -1]  # Last column (target variable)\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c303c4f",
   "metadata": {},
   "source": [
    "## **Task A Split Ratio 70:30**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee341f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training samples: 2217\n",
      "Testing samples: 951\n",
      "\n",
      "‚úì Data split completed\n",
      "‚úì Feature scaling applied (StandardScaler)\n"
     ]
    }
   ],
   "source": [
    "# Split the data 70:30\n",
    "X_train_70, X_test_70, y_train_70, y_test_70 = train_test_split(\n",
    "    X, y, train_size=0.7, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train_70)}\")\n",
    "print(f\"Testing samples: {len(X_test_70)}\")\n",
    "\n",
    "# Feature Scaling (IMPORTANT for SVM!)\n",
    "scaler_70 = StandardScaler()\n",
    "X_train_70_scaled = scaler_70.fit_transform(X_train_70)\n",
    "X_test_70_scaled = scaler_70.transform(X_test_70)\n",
    "\n",
    "print(\"\\n‚úì Data split completed\")\n",
    "print(\"‚úì Feature scaling applied (StandardScaler)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72438d14",
   "metadata": {},
   "source": [
    "#### **a. Linear Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e514f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM with linear kernel...\n",
      "‚úì Training completed\n",
      "\n",
      "Accuracy: 0.9790 (97.90%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[463  13]\n",
      " [  7 468]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.99      0.97      0.98       476\n",
      "        male       0.97      0.99      0.98       475\n",
      "\n",
      "    accuracy                           0.98       951\n",
      "   macro avg       0.98      0.98      0.98       951\n",
      "weighted avg       0.98      0.98      0.98       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train SVM with linear kernel\n",
    "svm_linear_70 = SVC(kernel=\"linear\", random_state=42)\n",
    "print(\"\\nTraining SVM with linear kernel...\")\n",
    "svm_linear_70.fit(X_train_70_scaled, y_train_70)\n",
    "print(\"‚úì Training completed\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear_70 = svm_linear_70.predict(X_test_70_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_linear_70 = accuracy_score(y_test_70, y_pred_linear_70)\n",
    "print(f\"\\nAccuracy: {accuracy_linear_70:.4f} ({accuracy_linear_70 * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_70, y_pred_linear_70)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_70, y_pred_linear_70))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab4b91",
   "metadata": {},
   "source": [
    "#### **b. Polynomial Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a81a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM with polynomial kernel (degree=3)...\n",
      "‚úì Training completed\n",
      "\n",
      "Accuracy: 0.9600 (96.00%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[443  33]\n",
      " [  5 470]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.99      0.93      0.96       476\n",
      "        male       0.93      0.99      0.96       475\n",
      "\n",
      "    accuracy                           0.96       951\n",
      "   macro avg       0.96      0.96      0.96       951\n",
      "weighted avg       0.96      0.96      0.96       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train SVM with polynomial kernel (degree=3 is default)\n",
    "svm_poly_70 = SVC(kernel=\"poly\", degree=3, random_state=42)\n",
    "print(\"\\nTraining SVM with polynomial kernel (degree=3)...\")\n",
    "svm_poly_70.fit(X_train_70_scaled, y_train_70)\n",
    "print(\"‚úì Training completed\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_poly_70 = svm_poly_70.predict(X_test_70_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_poly_70 = accuracy_score(y_test_70, y_pred_poly_70)\n",
    "print(f\"\\nAccuracy: {accuracy_poly_70:.4f} ({accuracy_poly_70 * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_70, y_pred_poly_70)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_70, y_pred_poly_70))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0213a6",
   "metadata": {},
   "source": [
    "#### **c. RBF Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429a2939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM with RBF kernel...\n",
      "‚úì Training completed\n",
      "\n",
      "Accuracy: 0.9832 (98.32%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[467   9]\n",
      " [  7 468]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.99      0.98      0.98       476\n",
      "        male       0.98      0.99      0.98       475\n",
      "\n",
      "    accuracy                           0.98       951\n",
      "   macro avg       0.98      0.98      0.98       951\n",
      "weighted avg       0.98      0.98      0.98       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train SVM with RBF kernel\n",
    "svm_rbf_70 = SVC(kernel=\"rbf\", random_state=42)\n",
    "print(\"\\nTraining SVM with RBF kernel...\")\n",
    "svm_rbf_70.fit(X_train_70_scaled, y_train_70)\n",
    "print(\"‚úì Training completed\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rbf_70 = svm_rbf_70.predict(X_test_70_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_rbf_70 = accuracy_score(y_test_70, y_pred_rbf_70)\n",
    "print(f\"\\nAccuracy: {accuracy_rbf_70:.4f} ({accuracy_rbf_70 * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_70, y_pred_rbf_70)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_70, y_pred_rbf_70))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c191ba6",
   "metadata": {},
   "source": [
    "## **Task B Split Ratio 80:20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a1aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training samples: 2534\n",
      "Testing samples: 634\n",
      "\n",
      "‚úì Data split completed\n",
      "‚úì Feature scaling applied (StandardScaler)\n"
     ]
    }
   ],
   "source": [
    "# Split the data 80:20\n",
    "X_train_80, X_test_80, y_train_80, y_test_80 = train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train_80)}\")\n",
    "print(f\"Testing samples: {len(X_test_80)}\")\n",
    "\n",
    "# Feature Scaling\n",
    "scaler_80 = StandardScaler()\n",
    "X_train_80_scaled = scaler_80.fit_transform(X_train_80)\n",
    "X_test_80_scaled = scaler_80.transform(X_test_80)\n",
    "\n",
    "print(\"\\n‚úì Data split completed\")\n",
    "print(\"‚úì Feature scaling applied (StandardScaler)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6783e7",
   "metadata": {},
   "source": [
    "#### **a. Linear Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "612c8584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM with linear kernel...\n",
      "‚úì Training completed\n",
      "\n",
      "Accuracy: 0.9748 (97.48%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[306  11]\n",
      " [  5 312]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.98      0.97      0.97       317\n",
      "        male       0.97      0.98      0.97       317\n",
      "\n",
      "    accuracy                           0.97       634\n",
      "   macro avg       0.97      0.97      0.97       634\n",
      "weighted avg       0.97      0.97      0.97       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train SVM with linear kernel\n",
    "svm_linear_80 = SVC(kernel=\"linear\", random_state=42)\n",
    "print(\"\\nTraining SVM with linear kernel...\")\n",
    "svm_linear_80.fit(X_train_80_scaled, y_train_80)\n",
    "print(\"‚úì Training completed\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear_80 = svm_linear_80.predict(X_test_80_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_linear_80 = accuracy_score(y_test_80, y_pred_linear_80)\n",
    "print(f\"\\nAccuracy: {accuracy_linear_80:.4f} ({accuracy_linear_80 * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_80, y_pred_linear_80)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_80, y_pred_linear_80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d9bef",
   "metadata": {},
   "source": [
    "#### **b. Polynomial Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e64ee4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM with polynomial kernel (degree=3)...\n",
      "‚úì Training completed\n",
      "\n",
      "Accuracy: 0.9558 (95.58%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[292  25]\n",
      " [  3 314]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.99      0.92      0.95       317\n",
      "        male       0.93      0.99      0.96       317\n",
      "\n",
      "    accuracy                           0.96       634\n",
      "   macro avg       0.96      0.96      0.96       634\n",
      "weighted avg       0.96      0.96      0.96       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train SVM with polynomial kernel\n",
    "svm_poly_80 = SVC(kernel=\"poly\", degree=3, random_state=42)\n",
    "print(\"\\nTraining SVM with polynomial kernel (degree=3)...\")\n",
    "svm_poly_80.fit(X_train_80_scaled, y_train_80)\n",
    "print(\"‚úì Training completed\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_poly_80 = svm_poly_80.predict(X_test_80_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_poly_80 = accuracy_score(y_test_80, y_pred_poly_80)\n",
    "print(f\"\\nAccuracy: {accuracy_poly_80:.4f} ({accuracy_poly_80 * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_80, y_pred_poly_80)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_80, y_pred_poly_80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761405e",
   "metadata": {},
   "source": [
    "#### **c. RBF Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dacda1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM with RBF kernel...\n",
      "‚úì Training completed\n",
      "\n",
      "Accuracy: 0.9826 (98.26%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[310   7]\n",
      " [  4 313]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.99      0.98      0.98       317\n",
      "        male       0.98      0.99      0.98       317\n",
      "\n",
      "    accuracy                           0.98       634\n",
      "   macro avg       0.98      0.98      0.98       634\n",
      "weighted avg       0.98      0.98      0.98       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train SVM with RBF kernel\n",
    "svm_rbf_80 = SVC(kernel=\"rbf\", random_state=42)\n",
    "print(\"\\nTraining SVM with RBF kernel...\")\n",
    "svm_rbf_80.fit(X_train_80_scaled, y_train_80)\n",
    "print(\"‚úì Training completed\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rbf_80 = svm_rbf_80.predict(X_test_80_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_rbf_80 = accuracy_score(y_test_80, y_pred_rbf_80)\n",
    "print(f\"\\nAccuracy: {accuracy_rbf_80:.4f} ({accuracy_rbf_80 * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_80, y_pred_rbf_80)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_80, y_pred_rbf_80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2239ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "425353c9",
   "metadata": {},
   "source": [
    "### **Task C Performance Comparison Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37f7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "SECTION 1.3: PERFORMANCE COMPARISON TABLE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 1.3: PERFORMANCE COMPARISON TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create results table\n",
    "results = [\n",
    "    {\n",
    "        \"Split Ratio\": \"70:30\",\n",
    "        \"Kernel\": \"Linear\",\n",
    "        \"Accuracy\": accuracy_linear_70,\n",
    "        \"Accuracy (%)\": f\"{accuracy_linear_70 * 100:.2f}%\",\n",
    "    },\n",
    "    {\n",
    "        \"Split Ratio\": \"70:30\",\n",
    "        \"Kernel\": \"Polynomial\",\n",
    "        \"Accuracy\": accuracy_poly_70,\n",
    "        \"Accuracy (%)\": f\"{accuracy_poly_70 * 100:.2f}%\",\n",
    "    },\n",
    "    {\n",
    "        \"Split Ratio\": \"70:30\",\n",
    "        \"Kernel\": \"RBF\",\n",
    "        \"Accuracy\": accuracy_rbf_70,\n",
    "        \"Accuracy (%)\": f\"{accuracy_rbf_70 * 100:.2f}%\",\n",
    "    },\n",
    "    {\n",
    "        \"Split Ratio\": \"80:20\",\n",
    "        \"Kernel\": \"Linear\",\n",
    "        \"Accuracy\": accuracy_linear_80,\n",
    "        \"Accuracy (%)\": f\"{accuracy_linear_80 * 100:.2f}%\",\n",
    "    },\n",
    "    {\n",
    "        \"Split Ratio\": \"80:20\",\n",
    "        \"Kernel\": \"Polynomial\",\n",
    "        \"Accuracy\": accuracy_poly_80,\n",
    "        \"Accuracy (%)\": f\"{accuracy_poly_80 * 100:.2f}%\",\n",
    "    },\n",
    "    {\n",
    "        \"Split Ratio\": \"80:20\",\n",
    "        \"Kernel\": \"RBF\",\n",
    "        \"Accuracy\": accuracy_rbf_80,\n",
    "        \"Accuracy (%)\": f\"{accuracy_rbf_80 * 100:.2f}%\",\n",
    "    },\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef59c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMPREHENSIVE ACCURACY COMPARISON:\n",
      "--------------------------------------------------------------------------------\n",
      "Split Ratio     Kernel  Accuracy Accuracy (%)\n",
      "      70:30     Linear  0.978970       97.90%\n",
      "      70:30 Polynomial  0.960042       96.00%\n",
      "      70:30        RBF  0.983176       98.32%\n",
      "      80:20     Linear  0.974763       97.48%\n",
      "      80:20 Polynomial  0.955836       95.58%\n",
      "      80:20        RBF  0.982650       98.26%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ BEST MODEL:\n",
      "   Split Ratio: 70:30\n",
      "   Kernel: RBF\n",
      "   Accuracy: 98.32%\n",
      "\n",
      "üìä SUMMARY STATISTICS:\n",
      "   Average Accuracy: 0.9726 (97.26%)\n",
      "   Best Accuracy: 0.9832 (98.32%)\n",
      "   Worst Accuracy: 0.9558 (95.58%)\n",
      "   Std Deviation: 0.0118\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCOMPREHENSIVE ACCURACY COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Find best performing model\n",
    "best_idx = results_df[\"Accuracy\"].idxmax()\n",
    "best_model = results_df.iloc[best_idx]\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL:\")\n",
    "print(f\"   Split Ratio: {best_model['Split Ratio']}\")\n",
    "print(f\"   Kernel: {best_model['Kernel']}\")\n",
    "print(f\"   Accuracy: {best_model['Accuracy (%)']}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä SUMMARY STATISTICS:\")\n",
    "print(\n",
    "    f\"   Average Accuracy: {results_df['Accuracy'].mean():.4f} ({results_df['Accuracy'].mean() * 100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Best Accuracy: {results_df['Accuracy'].max():.4f} ({results_df['Accuracy'].max() * 100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Worst Accuracy: {results_df['Accuracy'].min():.4f} ({results_df['Accuracy'].min() * 100:.2f}%)\"\n",
    ")\n",
    "print(f\"   Std Deviation: {results_df['Accuracy'].std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79714a07",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9fa1e0e",
   "metadata": {},
   "source": [
    "## Task 2 Day/Night Image Classification Using SVM-RBF With Histogram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f11bd156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b897c",
   "metadata": {},
   "source": [
    "### Step 1 - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cdc9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_histogram_features(image_path, bins=256):\n",
    "    \"\"\"\n",
    "    Extract color histogram features from an image.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the image file\n",
    "    bins : int\n",
    "        Number of bins for the histogram (default=256)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    features : numpy array\n",
    "        Flattened histogram features (bins x 3 channels = 768 features)\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Warning: Could not read {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Calculate histogram for each color channel (B, G, R)\n",
    "    features = []\n",
    "\n",
    "    for channel in range(3):  # 3 channels: Blue, Green, Red\n",
    "        # Calculate histogram for this channel\n",
    "        hist = cv2.calcHist([img], [channel], None, [bins], [0, 256])\n",
    "\n",
    "        # Flatten the histogram\n",
    "        hist = hist.flatten()\n",
    "\n",
    "        # Normalize the histogram (important!)\n",
    "        hist = hist / (hist.sum() + 1e-7)  # Add small value to avoid division by zero\n",
    "\n",
    "        # Add to features\n",
    "        features.extend(hist)\n",
    "\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf0f6c",
   "metadata": {},
   "source": [
    "### Step 2 - Load Image Datasets - Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f8c0277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING TRAINING DATASET\n",
      "================================================================================\n",
      "\n",
      "[STEP 2] Loading TRAINING dataset from: images/training\n",
      "\n",
      "Loading images from: images/training/day\n",
      "Found 120 day images\n",
      "  Processed 50/120 day images...\n",
      "  Processed 100/120 day images...\n",
      "\n",
      "Loading images from: images/training/night\n",
      "Found 120 night images\n",
      "  Processed 50/120 night images...\n",
      "  Processed 100/120 night images...\n",
      "\n",
      "================================================================================\n",
      "LOADING TEST DATASET\n",
      "================================================================================\n",
      "\n",
      "[STEP 2] Loading TEST dataset from: images/test\n",
      "\n",
      "Loading images from: images/test/day\n",
      "Found 80 day images\n",
      "  Processed 50/80 day images...\n",
      "\n",
      "Loading images from: images/test/night\n",
      "Found 80 night images\n",
      "  Processed 50/80 night images...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def load_image_dataset(base_dir, dataset_type=\"training\", bins=256):\n",
    "    \"\"\"\n",
    "    Load images from 'day' and 'night' subdirectories under training or test folders,\n",
    "    and extract histogram features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory containing 'training' and 'test' folders\n",
    "    dataset_type : str\n",
    "        Specify 'training' or 'test' to choose which dataset to load\n",
    "    bins : int\n",
    "        Number of histogram bins for feature extraction\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy array\n",
    "        Feature matrix (n_samples, n_features)\n",
    "    y : numpy array\n",
    "        Labels (0 = day, 1 = night)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    data_dir = os.path.join(base_dir, dataset_type)\n",
    "    print(f\"\\n[STEP 2] Loading {dataset_type.upper()} dataset from: {data_dir}\")\n",
    "\n",
    "    # Load DAY images (label = 0)\n",
    "    day_dir = os.path.join(data_dir, \"day\")\n",
    "    print(f\"\\nLoading images from: {day_dir}\")\n",
    "    if os.path.exists(day_dir):\n",
    "        day_images = [\n",
    "            f\n",
    "            for f in os.listdir(day_dir)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\"))\n",
    "        ]\n",
    "        print(f\"Found {len(day_images)} day images\")\n",
    "\n",
    "        for idx, img_file in enumerate(day_images, 1):\n",
    "            img_path = os.path.join(day_dir, img_file)\n",
    "            features = extract_histogram_features(img_path, bins)\n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                y.append(0)\n",
    "            if idx % 50 == 0:\n",
    "                print(f\"  Processed {idx}/{len(day_images)} day images...\")\n",
    "    else:\n",
    "        print(f\"ERROR: Directory not found: {day_dir}\")\n",
    "\n",
    "    # Load NIGHT images (label = 1)\n",
    "    night_dir = os.path.join(data_dir, \"night\")\n",
    "    print(f\"\\nLoading images from: {night_dir}\")\n",
    "    if os.path.exists(night_dir):\n",
    "        night_images = [\n",
    "            f\n",
    "            for f in os.listdir(night_dir)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\"))\n",
    "        ]\n",
    "        print(f\"Found {len(night_images)} night images\")\n",
    "\n",
    "        for idx, img_file in enumerate(night_images, 1):\n",
    "            img_path = os.path.join(night_dir, img_file)\n",
    "            features = extract_histogram_features(img_path, bins)\n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                y.append(1)\n",
    "            if idx % 50 == 0:\n",
    "                print(f\"  Processed {idx}/{len(night_images)} night images...\")\n",
    "    else:\n",
    "        print(f\"ERROR: Directory not found: {night_dir}\")\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Load both TRAINING and TEST datasets\n",
    "# ===============================\n",
    "BASE_DIR = \"images\"  # your main folder inside Week11\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOADING TRAINING DATASET\")\n",
    "print(\"=\" * 80)\n",
    "X_train, y_train = load_image_dataset(BASE_DIR, \"training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOADING TEST DATASET\")\n",
    "print(\"=\" * 80)\n",
    "X_test, y_test = load_image_dataset(BASE_DIR, \"test\")\n",
    "\n",
    "\n",
    "# Dataset Summary\n",
    "def dataset_summary(X, y, name):\n",
    "    print(f\"\\n‚úì {name.upper()} dataset loaded successfully!\")\n",
    "    print(f\"  Total images: {len(X)}\")\n",
    "    print(f\"  Day images (label=0): {np.sum(y == 0)}\")\n",
    "    print(f\"  Night images (label=1): {np.sum(y == 1)}\")\n",
    "    print(\n",
    "        f\"  Class balance: {np.sum(y == 0) / len(y) * 100:.1f}% day, {np.sum(y == 1) / len(y) * 100:.1f}% night\"\n",
    "    )\n",
    "\n",
    "\n",
    "dataset_summary(X_train, y_train, \"Training\")\n",
    "dataset_summary(X_test, y_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f06c42",
   "metadata": {},
   "source": [
    "### Step 3 - Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43518e97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(X)\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Day: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.sum(y_train\u001b[38;5;250m \u001b[39m==\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/machine-learning-kuliah/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/machine-learning-kuliah/.venv/lib/python3.13/site-packages/sklearn/model_selection/_split.py:2919\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2916\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2918\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2921\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/machine-learning-kuliah/.venv/lib/python3.13/site-packages/sklearn/model_selection/_split.py:2499\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2496\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2499\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2503\u001b[39m     )\n\u001b[32m   2505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=0.2 and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} images ({len(X_train) / len(X) * 100:.1f}%)\")\n",
    "print(f\"  Day: {np.sum(y_train == 0)}\")\n",
    "print(f\"  Night: {np.sum(y_train == 1)}\")\n",
    "\n",
    "print(f\"\\nTesting set: {len(X_test)} images ({len(X_test) / len(X) * 100:.1f}%)\")\n",
    "print(f\"  Day: {np.sum(y_test == 0)}\")\n",
    "print(f\"  Night: {np.sum(y_test == 1)}\")\n",
    "\n",
    "print(\"\\n‚úì Data split completed with stratification (maintains class balance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ba0c7",
   "metadata": {},
   "source": [
    "### Step 4 - Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úì Features standardized using StandardScaler\")\n",
    "print(f\"  Mean: ~0, Std: ~1 for each feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f9191",
   "metadata": {},
   "source": [
    "### Step 5 - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "print(\"\\nTraining baseline SVM-RBF model...\")\n",
    "svm_baseline = SVC(kernel=\"rbf\", random_state=42)\n",
    "svm_baseline.fit(X_train_scaled, y_train)\n",
    "print(\"‚úì Training completed\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = svm_baseline.predict(X_test_scaled)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"\\nBaseline Accuracy: {accuracy_baseline:.4f} ({accuracy_baseline * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
    "print(\"              Predicted\")\n",
    "print(\"              Day  Night\")\n",
    "print(f\"Actual Day    {cm_baseline[0][0]:3}   {cm_baseline[0][1]:3}\")\n",
    "print(f\"       Night  {cm_baseline[1][0]:3}   {cm_baseline[1][1]:3}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=[\"Day\", \"Night\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbcfcaa",
   "metadata": {},
   "source": [
    "### Step 6 - Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "gamma_values = [0.001, 0.01, 0.1, 1, \"scale\", \"auto\"]\n",
    "\n",
    "print(f\"\\nGrid Search:\")\n",
    "print(f\"  C values: {C_values}\")\n",
    "print(f\"  gamma values: {gamma_values}\")\n",
    "print(f\"  Total combinations: {len(C_values) * len(gamma_values)}\")\n",
    "\n",
    "# Store results\n",
    "tuning_results = []\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "best_model = None\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Testing combinations...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for C in C_values:\n",
    "    for gamma in gamma_values:\n",
    "        # Train model\n",
    "        svm = SVC(kernel=\"rbf\", C=C, gamma=gamma, random_state=42)\n",
    "        svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Store results\n",
    "        tuning_results.append({\n",
    "            \"C\": C,\n",
    "            \"gamma\": str(gamma),\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Accuracy (%)\": f\"{accuracy * 100:.2f}%\",\n",
    "        })\n",
    "\n",
    "        # Print result\n",
    "        print(\n",
    "            f\"C={C:6}, gamma={str(gamma):8} ‚Üí Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)\"\n",
    "        )\n",
    "\n",
    "        # Track best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {\"C\": C, \"gamma\": gamma}\n",
    "            best_model = svm\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c3825",
   "metadata": {},
   "source": [
    "### Step 7 - Best Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: BEST MODEL EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nBEST HYPERPARAMETERS:\")\n",
    "print(f\"   C: {best_params['C']}\")\n",
    "print(f\"   gamma: {best_params['gamma']}\")\n",
    "print(f\"   Best Accuracy: {best_accuracy:.4f} ({best_accuracy * 100:.2f}%)\")\n",
    "\n",
    "# Detailed evaluation of best model\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CONFUSION MATRIX (Best Model):\")\n",
    "print(\"-\" * 80)\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "print(\"              Predicted\")\n",
    "print(\"              Day  Night\")\n",
    "print(f\"Actual Day    {cm_best[0][0]:3}   {cm_best[0][1]:3}\")\n",
    "print(f\"       Night  {cm_best[1][0]:3}   {cm_best[1][1]:3}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CLASSIFICATION REPORT (Best Model):\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report(y_test, y_pred_best, target_names=[\"Day\", \"Night\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb283eb2",
   "metadata": {},
   "source": [
    "### Step 8 - Performance Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c83841",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Baseline (default)\",\n",
    "        \"C\": 1.0,\n",
    "        \"gamma\": \"scale\",\n",
    "        \"Accuracy\": accuracy_baseline,\n",
    "        \"Accuracy (%)\": f\"{accuracy_baseline * 100:.2f}%\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Best (tuned)\",\n",
    "        \"C\": best_params[\"C\"],\n",
    "        \"gamma\": best_params[\"gamma\"],\n",
    "        \"Accuracy\": best_accuracy,\n",
    "        \"Accuracy (%)\": f\"{best_accuracy * 100:.2f}%\",\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "improvement = (best_accuracy - accuracy_baseline) * 100\n",
    "print(f\"\\nüìà Improvement from tuning: {improvement:+.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d7020b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-kuliah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
